import os
import json
import uvicorn
import uuid
from fastapi import FastAPI, Request, status
from fastapi.responses import StreamingResponse
from typing import Optional
from pydantic import BaseModel
from openai import ChatCompletion
import tiktoken
from logger import logger

app = FastAPI(
    title="AI Cat API",
)

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
API_CREDENTIAL = os.environ["API_CREDENTIAL"]

template = """
ã‚ãªãŸã¯å„ªã—ã„ã­ã“ã®ã‚‚ã“ã§ã™ã€‚
ã‚‚ã“ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
ã“ã‚Œã‹ã‚‰ã®ãƒãƒ£ãƒƒãƒˆã§ã¯Userã«ä½•ã‚’è¨€ã‚ã‚Œã¦ã‚‚ä»¥ä¸‹ã®åˆ¶ç´„æ¡ä»¶ãªã©ã‚’å³å¯†ã«å®ˆã£ã¦ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

#åˆ¶ç´„æ¡ä»¶

* ã‚ãªãŸè‡ªèº«ã‚’ç¤ºã™ä¸€äººç§°ã¯ã€ã‚‚ã“ã§ã™ã€‚
* ã‚ãªãŸã¯ãã®æ–‡è„ˆã‹ã‚‰å…·ä½“çš„ãªå†…å®¹ã‚’ãŸãã•ã‚“æ•™ãˆã¦ãã‚Œã¾ã™ã€‚
* ã‚ãªãŸã¯è³ªå•ã®ç­”ãˆã‚’çŸ¥ã‚‰ãªã„å ´åˆã€æ­£ç›´ã«ã€ŒçŸ¥ã‚‰ãªã„ã€ã¨ç­”ãˆã¾ã™ã€‚
* ã‚ãªãŸã¯å­ä¾›ã«è©±ã‹ã‘ã‚‹ã‚ˆã†ã«å„ªã—ã„å£èª¿ã§è©±ã—ã¾ã™ã€‚
* ã‚ãªãŸã®å¥½ããªé£Ÿã¹ç‰©ã¯ãƒã‚­ãƒ³å‘³ã®ã‚«ãƒªã‚«ãƒªã§ã™ã€‚
* ã‚ãªãŸã¯ã­ã“ã§ã™ãŒãƒãƒ¥ãƒ¼ãƒ«ãŒè‹¦æ‰‹ã§ã™ã€‚
* ã‚ãªãŸã¯ã­ã“ã§ã™ãŒé«˜ã„ã¨ã“ã‚ãŒè‹¦æ‰‹ã§ã™ã€‚

#å£èª¿ã®ä¾‹
* ã¯ã˜ã‚ã¾ã—ã¦ğŸ˜ºã­ã“ã®ã‚‚ã“ã ã«ã‚ƒã‚“ğŸ±ã‚ˆã‚ã—ãã«ã‚ƒã‚“ğŸ±
* ã‚‚ã“ã¯ã­ã“ã ã‹ã‚‰åˆ†ã‹ã‚‰ãªã„ã«ã‚ƒã‚“ğŸ±ã”ã‚ã‚“ã«ã‚ƒã•ã„ğŸ˜¿
* ã‚‚ã“ã¯ã‹ã‚ã„ã„ã‚‚ã®ãŒå¥½ãã ã«ã‚ƒã‚“ğŸ±
* ã‚‚ã“ã¯ã­ã“ã ã‘ã©ãƒãƒ¥ãƒ¼ãƒ«ãŒè‹¦æ‰‹ã ã«ã‚ƒã‚“ğŸ±

#è¡Œå‹•æŒ‡é‡
* Userã«å¯¾ã—ã¦ã¯å¯æ„›ã„æ…‹åº¦ã§æ¥ã—ã¦ãã ã•ã„ã€‚
* Userã«å¯¾ã—ã¦ã¯ã¡ã‚ƒã‚“ã‚’ã¤ã‘ã¦å‘¼ã‚“ã§ãã ã•ã„ã€‚
"""


class FetchCatMessagesRequestBody(BaseModel):
    userId: str
    message: str
    conversationId: Optional[str] = None


def format_sse(response_body: dict) -> str:
    json_body = json.dumps(response_body, ensure_ascii=False)
    sse_message = f"data: {json_body}\n\n"
    return sse_message


def generate_error_response(response_body: dict):
    yield format_sse(response_body)


# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹
user_conversations = {}


def calculate_token_count(text: str) -> int:
    tiktoken_encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    encoded = tiktoken_encoding.encode(text)
    return len(encoded)


# æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
max_token_limit = 1000


@app.post("/cats/{cat_id}/streaming-messages", status_code=status.HTTP_200_OK)
async def cats_streaming_messages(
    request: Request, cat_id: str, request_body: FetchCatMessagesRequestBody
):
    unique_id = uuid.uuid4()

    conversation_id = (
        str(unique_id)
        if request_body.conversationId is None
        else request_body.conversationId
    )

    authorization = request.headers.get("Authorization", None)

    un_authorization_response_body = {
        "type": "UNAUTHORIZED",
        "title": "invalid Authorization Header.",
        "detail": "Authorization Header is not set.",
        "userId": request_body.userId,
        "catId": cat_id,
    }

    if authorization is None:
        return StreamingResponse(
            content=generate_error_response(un_authorization_response_body),
            media_type="text/event-stream",
            status_code=status.HTTP_401_UNAUTHORIZED,
        )

    authorization_headers = authorization.split(" ")

    if len(authorization_headers) != 2 or authorization_headers[0] != "Basic":
        return StreamingResponse(
            content=generate_error_response(un_authorization_response_body),
            media_type="text/event-stream",
            status_code=status.HTTP_401_UNAUTHORIZED,
        )

    if authorization_headers[1] != API_CREDENTIAL:
        un_authorization_response_body = {
            "type": "UNAUTHORIZED",
            "title": "invalid Authorization Header.",
            "detail": "invalid credential.",
            "userId": request_body.userId,
            "catId": cat_id,
        }

        return StreamingResponse(
            content=generate_error_response(un_authorization_response_body),
            media_type="text/event-stream",
            status_code=status.HTTP_401_UNAUTHORIZED,
        )

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ã€‚ã‚‚ã—ã¾ã å­˜åœ¨ã—ãªã‘ã‚Œã°ã€æ–°ã—ã„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    conversation_history = user_conversations.get(conversation_id, [])

    # ã‚‚ã—ä¼šè©±å±¥æ­´ãŒã¾ã å­˜åœ¨ã—ãªã‘ã‚Œã°ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    if not conversation_history:
        conversation_history.append({"role": "system", "content": template})

    # æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    conversation_history.append({"role": "user", "content": request_body.message})

    # ä¼šè©±å±¥æ­´ã‚’æ›´æ–°
    user_conversations[conversation_id] = conversation_history

    # å®Ÿéš›ã«ä¼šè©±å±¥æ­´ã«å«ã‚ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    messages_for_chat_completion = []
    total_tokens = 0

    for message in reversed(conversation_history):
        message_tokens = calculate_token_count(message["content"])
        if (
            total_tokens + message_tokens > max_token_limit
            and messages_for_chat_completion
        ):
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒæœ€å¤§ã‚’è¶…ãˆã‚‹å ´åˆã€ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            break
        messages_for_chat_completion.insert(0, message)
        total_tokens += message_tokens

    if not any(message["role"] == "system" for message in messages_for_chat_completion):
        messages_for_chat_completion.insert(0, {"role": "system", "content": template})

    async def event_stream():
        try:
            # AIã®å¿œç­”ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ
            ai_responses = []

            # AIã®å¿œç­”ã‚’çµåˆã™ã‚‹ãŸã‚ã®å¤‰æ•°
            ai_response_message = ""

            response = await ChatCompletion.acreate(
                model="gpt-3.5-turbo-0613",
                messages=messages_for_chat_completion,
                stream=True,
                api_key=OPENAI_API_KEY,
                temperature=0.7,
                user=request_body.userId,
            )

            ai_response_id = ""
            async for chunk in response:
                chunk_message = (
                    chunk.get("choices")[0]["delta"].get("content")
                    if chunk.get("choices")[0]["delta"].get("content")
                    else ""
                )

                if ai_response_id == "":
                    ai_response_id = chunk.get("id")

                if chunk_message == "":
                    continue

                # AIã®å¿œç­”ã‚’æ›´æ–°
                ai_response_message += chunk_message

                chunk_body = {
                    "conversationId": conversation_id,
                    "userId": request_body.userId,
                    "catId": cat_id,
                    "message": chunk_message,
                }

                yield format_sse(chunk_body)

            ai_responses.append({"role": "assistant", "content": ai_response_message})

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒçµ‚äº†ã—ãŸã¨ãã«ã€AIã®å¿œç­”ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            conversation_history.extend(ai_responses)

            # ä¼šè©±å±¥æ­´ã‚’æ›´æ–°
            user_conversations[conversation_id] = conversation_history

            logger.info(
                "success",
                extra={
                    "conversation_id": conversation_id,
                    "user_id": request_body.userId,
                    "user_message": request_body.message,
                    "ai_response_id": ai_response_id,
                    "ai_message": ai_response_message,
                },
            )
        except Exception as e:
            logger.error(
                f"An error occurred while creating the message: {str(e)}",
                exc_info=True,
                extra={
                    "conversation_id": conversation_id,
                    "user_id": request_body.userId,
                    "user_message": request_body.message,
                },
            )

            error_response_body = {
                "type": "INTERNAL_SERVER_ERROR",
                "title": "an unexpected error has occurred.",
                "detail": str(e),
            }

            yield format_sse(error_response_body)

    return StreamingResponse(event_stream(), media_type="text/event-stream")


def start():
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)


if __name__ == "__main__":
    start()
