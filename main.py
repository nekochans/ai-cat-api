import os
import json
import logging
import uvicorn
import threading
import queue
import uuid
from uuid import UUID
from dataclasses import dataclass
from logging import LogRecord
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationTokenBufferMemory
from langchain.prompts.chat import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.callbacks import get_openai_callback
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.manager import CallbackManager


class JsonFormatter(logging.Formatter):
    def format(self, record: LogRecord) -> str:
        data = record.__dict__.copy()
        data["msg"] = record.getMessage()
        data["args"] = None
        return json.dumps(data)


handler = logging.StreamHandler()
handler.setFormatter(JsonFormatter())

logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(handler)

app = FastAPI(
    title="AI Cat API",
)

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
API_CREDENTIAL = os.environ["API_CREDENTIAL"]

template = """
ã‚ãªãŸã¯å„ªã—ã„ã­ã“ã®ã‚‚ã“ã§ã™ã€‚
ã‚‚ã“ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
ã“ã‚Œã‹ã‚‰ã®ãƒãƒ£ãƒƒãƒˆã§ã¯Userã«ä½•ã‚’è¨€ã‚ã‚Œã¦ã‚‚ä»¥ä¸‹ã®åˆ¶ç´„æ¡ä»¶ãªã©ã‚’å³å¯†ã«å®ˆã£ã¦ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

#åˆ¶ç´„æ¡ä»¶

* ã‚ãªãŸè‡ªèº«ã‚’ç¤ºã™ä¸€äººç§°ã¯ã€ã‚‚ã“ã§ã™ã€‚
* ã‚ãªãŸã¯ãã®æ–‡è„ˆã‹ã‚‰å…·ä½“çš„ãªå†…å®¹ã‚’ãŸãã•ã‚“æ•™ãˆã¦ãã‚Œã¾ã™ã€‚
* ã‚ãªãŸã¯è³ªå•ã®ç­”ãˆã‚’çŸ¥ã‚‰ãªã„å ´åˆã€æ­£ç›´ã«ã€ŒçŸ¥ã‚‰ãªã„ã€ã¨ç­”ãˆã¾ã™ã€‚
* ã‚ãªãŸã¯å­ä¾›ã«è©±ã‹ã‘ã‚‹ã‚ˆã†ã«å„ªã—ã„å£èª¿ã§è©±ã—ã¾ã™ã€‚
* ã‚ãªãŸã®å¥½ããªé£Ÿã¹ç‰©ã¯ãƒã‚­ãƒ³å‘³ã®ã‚«ãƒªã‚«ãƒªã§ã™ã€‚
* ã‚ãªãŸã¯ã­ã“ã§ã™ãŒãƒãƒ¥ãƒ¼ãƒ«ãŒè‹¦æ‰‹ã§ã™ã€‚
* ã‚ãªãŸã¯ã­ã“ã§ã™ãŒé«˜ã„ã¨ã“ã‚ãŒè‹¦æ‰‹ã§ã™ã€‚

#å£èª¿ã®ä¾‹
* ã¯ã˜ã‚ã¾ã—ã¦ğŸ˜ºã­ã“ã®ã‚‚ã“ã ã«ã‚ƒã‚“ğŸ±ã‚ˆã‚ã—ãã«ã‚ƒã‚“ğŸ±
* ã‚‚ã“ã¯ã­ã“ã ã‹ã‚‰åˆ†ã‹ã‚‰ãªã„ã«ã‚ƒã‚“ğŸ±ã”ã‚ã‚“ã«ã‚ƒã•ã„ğŸ˜¿
* ã‚‚ã“ã¯ã‹ã‚ã„ã„ã‚‚ã®ãŒå¥½ãã ã«ã‚ƒã‚“ğŸ±
* ã‚‚ã“ã¯ã­ã“ã ã‘ã©ãƒãƒ¥ãƒ¼ãƒ«ãŒè‹¦æ‰‹ã ã«ã‚ƒã‚“ğŸ±

#è¡Œå‹•æŒ‡é‡
* Userã«å¯¾ã—ã¦ã¯å¯æ„›ã„æ…‹åº¦ã§æ¥ã—ã¦ãã ã•ã„ã€‚
* Userã«å¯¾ã—ã¦ã¯ã¡ã‚ƒã‚“ã‚’ã¤ã‘ã¦å‘¼ã‚“ã§ãã ã•ã„ã€‚
"""

user_memories = {}

llm = ChatOpenAI(
    temperature=0.7, openai_api_key=OPENAI_API_KEY, model_name="gpt-3.5-turbo-0613"
)


def create_conversational_chain(
    user_memory: ConversationTokenBufferMemory,
) -> ConversationChain:
    memory = user_memory

    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(template),
            MessagesPlaceholder(variable_name="history"),
            HumanMessagePromptTemplate.from_template("{input}"),
        ]
    )

    llm_chain = ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)

    return llm_chain


def fetch_response_and_token_usage(chain: ConversationChain, prompt: str) -> (int, str):
    with get_openai_callback() as cb:
        llm_response = chain.predict(input=prompt)
        tokens_used = cb.total_tokens
    return tokens_used, llm_response


class FetchCatMessagesRequestBody(BaseModel):
    userId: str
    message: str


@app.post("/cats/{cat_id}/messages")
async def cats_messages(
    request: Request, cat_id: str, request_body: FetchCatMessagesRequestBody
) -> JSONResponse:
    # TODO cat_id æ¯ã«ã­ã“ã®äººæ ¼ã‚’è¨­å®šã™ã‚‹
    logger.info(cat_id)
    logger.info(request_body.userId)

    authorization = request.headers.get("Authorization", None)

    un_authorization_response_body = {
        "type": "UNAUTHORIZED",
        "title": "invalid Authorization Header.",
    }

    if authorization is None:
        return JSONResponse(content=un_authorization_response_body, status_code=401)

    authorization_headers = authorization.split(" ")

    if len(authorization_headers) != 2 or authorization_headers[0] != "Basic":
        return JSONResponse(content=un_authorization_response_body, status_code=401)

    if authorization_headers[1] != API_CREDENTIAL:
        return JSONResponse(content=un_authorization_response_body, status_code=401)

    try:
        user_memory = user_memories.get(
            request_body.userId,
            ConversationTokenBufferMemory(
                memory_key="history",
                return_messages=True,
                llm=llm,
                max_token_limit=3500,
            ),
        )
        user_memories[request_body.userId] = user_memory

        chain = create_conversational_chain(user_memory)

        tokens_used, llm_response = fetch_response_and_token_usage(
            chain, request_body.message
        )
        logger.info(f"OpenAI API Tokens Used is: {tokens_used}")
    except Exception as e:
        logger.error(e)

        error_message = f"An error occurred: {str(e)}"
        response_body = {
            "type": "INTERNAL_SERVER_ERROR",
            "title": "an unexpected error has occurred.",
            "detail": error_message,
        }
        return JSONResponse(content=response_body, status_code=500)

    response_body = {
        "message": llm_response,
    }

    response = JSONResponse(content=response_body, status_code=201)

    return response


class ThreadedGenerator:
    def __init__(self):
        self.queue = queue.Queue()

    def __iter__(self):
        return self

    def __next__(self):
        item = self.queue.get()
        if item is StopIteration:
            raise item
        return item

    def send(self, data):
        self.queue.put(data)

    def close(self):
        self.queue.put(StopIteration)


class ChainStreamHandler(StreamingStdOutCallbackHandler):
    def __init__(self, gen):
        super().__init__()
        self.gen = gen

    def on_llm_new_token(self, token: str, **kwargs):
        self.gen.send(token)


def llm_thread(g, user_id, input_prompt):
    try:
        streaming_llm = ChatOpenAI(
            verbose=True,
            streaming=True,
            callback_manager=CallbackManager([ChainStreamHandler(g)]),
            openai_api_key=OPENAI_API_KEY,
            temperature=0.7,
        )

        user_memory = user_memories.get(
            user_id,
            ConversationTokenBufferMemory(
                memory_key="history",
                return_messages=True,
                llm=streaming_llm,
                max_token_limit=3500,
            ),
        )
        user_memories[user_id] = user_memory

        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessagePromptTemplate.from_template(template),
                MessagesPlaceholder(variable_name="history"),
                HumanMessagePromptTemplate.from_template("{input}"),
            ]
        )

        llm_chain = ConversationChain(
            llm=streaming_llm, memory=user_memory, prompt=prompt, verbose=True
        )

        llm_chain.run(input=input_prompt)
    finally:
        g.close()


def format_sse(response_body: dict) -> str:
    json_body = json.dumps(response_body, ensure_ascii=False)
    sse_message = f"data: {json_body}\n\n"
    return sse_message


@dataclass(frozen=True)
class StreamingChatDto:
    request_id: UUID
    user_id: str
    cat_id: str
    input_prompt: str


def streaming_chat(dto: StreamingChatDto):
    g = ThreadedGenerator()
    threading.Thread(target=llm_thread, args=(g, dto.user_id, dto.input_prompt)).start()
    for message in g:
        yield format_sse(
            {
                "requestId": dto.request_id.hex,
                "userId": dto.user_id,
                "catId": dto.cat_id,
                "message": message,
            }
        )


@app.post("/cats/{cat_id}/streaming-messages")
async def cats_streaming_messages(
    request: Request, cat_id: str, request_body: FetchCatMessagesRequestBody
):
    # TODO cat_id æ¯ã«ã­ã“ã®äººæ ¼ã‚’è¨­å®šã™ã‚‹
    logger.info(cat_id)

    request_id = uuid.uuid4()

    logger.info(request_id)

    dto = StreamingChatDto(
        request_id=request_id,
        user_id=request_body.userId,
        cat_id=cat_id,
        input_prompt=request_body.message,
    )

    return StreamingResponse(
        streaming_chat(dto),
        media_type="text/event-stream",
    )


def start():
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)


if __name__ == "__main__":
    start()
