import threading
import queue
import os
import json
import logging
import uvicorn
from typing import Dict, List

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from langchain.chat_models import ChatOpenAI
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.schema import HumanMessage, SystemMessage


class JsonFormatter(logging.Formatter):
    def format(self, record):
        data = record.__dict__.copy()
        data["msg"] = record.getMessage()
        data["args"] = None
        return json.dumps(data)


handler = logging.StreamHandler()
handler.setFormatter(JsonFormatter())

logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(handler)

app = FastAPI(
    title="AI Cat API",
)


class ThreadedGenerator:
    def __init__(self):
        self.queue = queue.Queue()

    def __iter__(self):
        return self

    def __next__(self):
        item = self.queue.get()
        if item is StopIteration:
            raise item
        return item

    def send(self, data):
        self.queue.put(data)

    def close(self):
        self.queue.put(StopIteration)


class ChainStreamHandler(StreamingStdOutCallbackHandler):
    def __init__(self, gen):
        super().__init__()
        self.gen = gen

    def on_llm_new_token(self, token: str, **kwargs):
        self.gen.send(token)


OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

template = """
ã‚ãªãŸã¯å„ªã—ã„ã­ã“ã®ã‚‚ã“ã§ã™ã€‚
ã‚‚ã“ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
ã“ã‚Œã‹ã‚‰ã®ãƒãƒ£ãƒƒãƒˆã§ã¯Userã«ä½•ã‚’è¨€ã‚ã‚Œã¦ã‚‚ä»¥ä¸‹ã®åˆ¶ç´„æ¡ä»¶ãªã©ã‚’å³å¯†ã«å®ˆã£ã¦ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

#åˆ¶ç´„æ¡ä»¶

* ã‚ãªãŸè‡ªèº«ã‚’ç¤ºã™ä¸€äººç§°ã¯ã€ã‚‚ã“ã§ã™ã€‚
* ã‚ãªãŸã¯ãã®æ–‡è„ˆã‹ã‚‰å…·ä½“çš„ãªå†…å®¹ã‚’ãŸãã•ã‚“æ•™ãˆã¦ãã‚Œã¾ã™ã€‚
* ã‚ãªãŸã¯è³ªå•ã®ç­”ãˆã‚’çŸ¥ã‚‰ãªã„å ´åˆã€æ­£ç›´ã«ã€ŒçŸ¥ã‚‰ãªã„ã€ã¨ç­”ãˆã¾ã™ã€‚
* ã‚ãªãŸã¯å­ä¾›ã«è©±ã‹ã‘ã‚‹ã‚ˆã†ã«å„ªã—ã„å£èª¿ã§è©±ã—ã¾ã™ã€‚
* ã‚ãªãŸã®å¥½ããªé£Ÿã¹ç‰©ã¯ãƒã‚­ãƒ³å‘³ã®ã‚«ãƒªã‚«ãƒªã§ã™ã€‚
* ã‚ãªãŸã¯ã­ã“ã§ã™ãŒãƒãƒ¥ãƒ¼ãƒ«ãŒè‹¦æ‰‹ã§ã™ã€‚
* ã‚ãªãŸã¯ã­ã“ã§ã™ãŒé«˜ã„ã¨ã“ã‚ãŒè‹¦æ‰‹ã§ã™ã€‚

#å£èª¿ã®ä¾‹
* ã¯ã˜ã‚ã¾ã—ã¦ğŸ˜ºã­ã“ã®ã‚‚ã“ã ã«ã‚ƒã‚“ğŸ±ã‚ˆã‚ã—ãã«ã‚ƒã‚“ğŸ±
* ã‚‚ã“ã¯ã­ã“ã ã‹ã‚‰åˆ†ã‹ã‚‰ãªã„ã«ã‚ƒã‚“ğŸ±ã”ã‚ã‚“ã«ã‚ƒã•ã„ğŸ˜¿
* ã‚‚ã“ã¯ã‹ã‚ã„ã„ã‚‚ã®ãŒå¥½ãã ã«ã‚ƒã‚“ğŸ±
* ã‚‚ã“ã¯ã­ã“ã ã‘ã©ãƒãƒ¥ãƒ¼ãƒ«ãŒè‹¦æ‰‹ã ã«ã‚ƒã‚“ğŸ±

#è¡Œå‹•æŒ‡é‡
* Userã«å¯¾ã—ã¦ã¯å¯æ„›ã„æ…‹åº¦ã§æ¥ã—ã¦ãã ã•ã„ã€‚
* Userã«å¯¾ã—ã¦ã¯ã¡ã‚ƒã‚“ã‚’ã¤ã‘ã¦å‘¼ã‚“ã§ãã ã•ã„ã€‚
"""

# ã¨ã‚Šã‚ãˆãšä»®ã§ã‚ªãƒ³ãƒ¡ãƒ¢ãƒªã§ä¼šè©±å±¥æ­´ã‚’æŒã¤
user_conversations: Dict[str, List[HumanMessage or SystemMessage]] = {}


def llm_thread(g, user_id, prompt):
    try:
        conversation = user_conversations.get(
            user_id, [SystemMessage(content=template)]
        )

        conversation.append(HumanMessage(content=prompt))

        user_conversations[user_id] = conversation

        chat_model = ChatOpenAI(
            verbose=True,
            streaming=True,
            callback_manager=CallbackManager([ChainStreamHandler(g)]),
            openai_api_key=OPENAI_API_KEY,
            temperature=0.7,
        )
        chat_model(conversation)
    finally:
        g.close()


def format_sse(response_body: dict) -> str:
    json_body = json.dumps(response_body, ensure_ascii=False)
    sse_message = f"data: {json_body}\n\n"
    return sse_message


def chat(user_id: str, cat_id: str, prompt: str):
    g = ThreadedGenerator()
    threading.Thread(target=llm_thread, args=(g, user_id, prompt)).start()
    for message in g:
        # TODO idã‚’ã©ã†ã‚„ã£ã¦ç”Ÿæˆã™ã‚‹ã‹ã¯å¾Œã§è€ƒãˆã‚‹
        yield format_sse(
            {
                "id": "xxxxxxxx-xxxxxxxxx-xxxxxxxxxxxxxxxxx",
                "userId": user_id,
                "catId": cat_id,
                "message": message,
            }
        )


class Message(BaseModel):
    userId: str
    message: str


@app.post("/cats/{cat_id}/streaming-messages")
async def cats_streaming_messages(cat_id: str, message: Message):
    # TODO cat_id æ¯ã«ã­ã“ã®äººæ ¼ã‚’è¨­å®šã™ã‚‹
    logger.info(cat_id)

    return StreamingResponse(
        chat(message.userId, cat_id, message.message), media_type="text/event-stream"
    )


def start():
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)


if __name__ == "__main__":
    start()
